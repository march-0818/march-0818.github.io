define({ entries : {
    "2016Statistical": {
        "author": " Shu, Jie  and  Dolman, G. E.  and  Duan, Jiang  and  Qiu, Guoping  and  Ilyas, Mohammad",
        "journal": "Biomedical Engineering Online",
        "number": "1",
        "pages": "46",
        "title": "Statistical colour models: an automated digital image analysis method for quantification of histological biomarkers",
        "type": "article",
        "volume": "15",
        "year": "2016"
    },
    "8265451": {
        "author": "Pound, Michael P. and Atkinson, Jonathan A. and Wells, Darren M. and Pridmore, Tony P. and French, Andrew P.",
        "booktitle": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)",
        "doi": "10.1109/ICCVW.2017.241",
        "number": "",
        "pages": "2055-2063",
        "title": "Deep Learning for Multi-task Plant Phenotyping",
        "type": "INPROCEEDINGS",
        "volume": "",
        "year": "2017"
    },
    "CHAKRAVARTULA2023107654": {
        "abstract": "Convective dryer embedded with computer vision (CV) system and load cell was used to continuously monitor carrot slices that are either unblanched or blanched (90\u00a0\u00b0C for 2\u00a0min) during product drying (35\u00a0\u00b0C, 35\u00a0% R.H., 3\u00a0m\u00a0s\u22121 airflow). The CV system and load cell were selected as in-line Process Analytical Technology tools within a proactive Quality-by-Design framework and embedded for, i) monitoring of product features (i.e., weight, colour, and size); and ii) developing shrinkage-dependent moisture prediction models using linear regression. The evaluated shrinkage-dependent linear models showed superior performances (RMSE, 0.005\u20130.007) benchmarked against selected thin-layer models of increasing complexity. The study tested a smart-enabled prototype dryer with the potential for automation and integrating proactive quality strategies.",
        "author": "Swathi Sirisha Nallan Chakravartula and Andrea Bandiera and Marco Nardella and Giacomo Bedini and Pietro Ibba and Riccardo Massantini and Roberto Moscetti",
        "doi": "https://doi.org/10.1016/j.compag.2023.107654",
        "issn": "0168-1699",
        "journal": "Computers and Electronics in Agriculture",
        "keywords": "Process analytical technology, Quality-by-design, Computer vision,  L., Segmented-linear model",
        "pages": "107654",
        "title": "Computer vision-based smart monitoring and control system for food drying: A study on carrot slices",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S016816992300042X",
        "volume": "206",
        "year": "2023"
    },
    "DAIRATH2023100210": {
        "abstract": "Huge harvest and post-harvest losses in conventional processes and methods demands precise applications of agricultural inputs. Computer vision based robotic picking is one of such systems that reduces time consumption, labor requirement; and enhances fruits picking and grading efficiency. In this study, an all-in-one prototype robotic picking cum grading system is developed that integrates robotic picking and fruit quality recognition based grading processes. Therein, computer vision algorithms are used for fruit detection followed by picking and placing through controlled robotic arm. For fruit detection, the steps includes color scheme conversion, masking of normal/fresh skin, masking of defects, and morpological dilation operations; and opencv library is used for implementation in Python Language on Anaconda Spyder Integrated Development Environment (IDE). The detection information is then communicated to robotic arm through microcontroller. The fabricated arm is of 4 Degrees of Freedom (4-DOF) and therein, four servo motors aid the robotic arm. The time taken by proposed computer vision algotihms scheme is as follows: 0.0035136\u00a0s, 0.0081954\u00a0s, 0.0005433\u00a0s, and 0.0002408\u00a0s for color scheme conversion, segmentation, morphological operations, and bitwise operations respectively;\u00a0 and total time for complete scheme is 0.0125\u00a0s. Overall, the developed system took average time of 15\u00a0s for one grading cycle in case of poor quality fruits. For this case, no action is carried out and the fruit passes the conveyor directly to storage bin. However, the time required for good quality fruits is high i.e. 21\u00a0s as the robotic arm picks and places the fruits for this case.",
        "author": "Meer Hannan Dairath and M. Waqar Akram and M. Ahmad Mehmood and H. Umair Sarwar and M. Zuhaib Akram and M. Mubashar Omar and M. Faheem",
        "doi": "https://doi.org/10.1016/j.atech.2023.100210",
        "issn": "2772-3755",
        "journal": "Smart Agricultural Technology",
        "keywords": "Computer/machine vision, Fruit image detection, Robotic picking, Fruit grading, Microcontroller",
        "pages": "100210",
        "title": "Computer vision-based prototype robotic picking cum grading system for fruits",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2772375523000400",
        "volume": "4",
        "year": "2023"
    },
    "JIA2022100984": {
        "abstract": "To evaluate and predict the freshness of salmon nondestructively, a computer vision technology was developed based on eye color to predict multiple freshness indicators of salmon simultaneously during storage at 0\u00a0\u00b0C. The RGB, L*a*b*\u00a0, and HSI color spaces of eye images were analyzed by an image processing algorithm. It is demonstrated that the eye color parameters R, G, B, L*\u00a0, I, and \u0394E were correlated with freshness indicators to establish the multiple linear regression (MLR) and support vector regression (SVR) models. The MLR models outperformed SVR models with high correlation coefficients R2, F value, and low relative errors. The achieved results showed that it was a nondestructive, fast method for predicting the freshness of salmon stored at 0\u00a0\u00b0C by evaluating the eye color parameters with computer vision.",
        "author": "Zhixin Jia and Meng Li and Ce Shi and Jiaran Zhang and Xinting Yang",
        "doi": "https://doi.org/10.1016/j.fpsl.2022.100984",
        "issn": "2214-2894",
        "journal": "Food Packaging and Shelf Life",
        "keywords": "Computer vision\u00b7Salmon\u00b7Eye color parameters\u00b7Chilled storage\u00b7Freshness indicators",
        "pages": "100984",
        "title": "Determination of salmon freshness by computer vision based on eye color",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2214289422001764",
        "volume": "34",
        "year": "2022"
    },
    "MORAL202259": {
        "abstract": "This paper describes the scientific achievements of a collaboration between a research group and the waste management division of a company. While these results might be the basis for several practical or commercial developments, we here focus on a novel scientific contribution: a methodology to automatically generate geo-located waste container maps. It is based on the use of Computer Vision algorithms to detect waste containers and identify their geographic location and dimensions. Algorithms analyze a video sequence and provide an automatic discrimination between images with and without containers. More precisely, two state-of-the-art object detectors based on deep learning techniques have been selected for testing, according to their performance and to their adaptability to an on-board real-time environment: EfficientDet and YOLOv5. Experimental results indicate that the proposed visual model for waste container detection is able to effectively operate with consistent performance disregarding the container type (organic waste, plastic, glass and paper recycling,\u2026) and the city layout, which has been assessed by evaluating it on eleven different Spanish cities that vary in terms of size, climate, urban layout and containers\u2019 appearance.",
        "author": "Paula Moral and \u00c1lvaro Garc\u00eda-Mart\u00edn and Marcos Escudero-Vi\u00f1olo and Jos\u00e9 M. Mart\u00ednez and Jes\u00fas Besc\u00f3s and Jes\u00fas Pe\u00f1uela and Juan Carlos Mart\u00ednez and Gonzalo Alvis",
        "doi": "https://doi.org/10.1016/j.wasman.2022.08.007",
        "issn": "0956-053X",
        "journal": "Waste Management",
        "keywords": "Waste container localization, Deep Learning, Computer Vision, Object detection",
        "pages": "59-68",
        "title": "Towards automatic waste containers management in cities via computer vision: containers localization and geo-positioning in city maps",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S0956053X22004093",
        "volume": "152",
        "year": "2022"
    },
    "REIFS2023101185": {
        "abstract": "One of the most important challenges in the management and treatment of complex wounds is the observation and measurement of different indicators that can be observed on the wound over time. This article will present the idea of addressing this challenge with the use of images captured on a mobile device. The aim of this work is to evaluate the use of digitization systems in the field of chronic wound management as tools that support the professional in improving patient care and decision making, as well as to use computer vision and artificial intelligence to improve wound assessment. An approach based on visual recognition and a classification system is proposed; visual recognition using superpixel techniques to determine the region of interest of the wound, as well as calculating its area and a classification system based on convolutional networks to classify its tissues. We found that our proposed approach, Visual Computing methods to detect Wound contour and measurement (with a Median Relative Error of 2.907 and inter-rater reliability of 0.98%) and Tissue Classification CNN with excellent results using Resnet50 with 0.85 of accuracy.",
        "author": "David Reifs and Lorena Casanova-Lozano and Ramon Reig-Bola\u00f1o and Sergi Grau-Carrion",
        "doi": "https://doi.org/10.1016/j.imu.2023.101185",
        "issn": "2352-9148",
        "journal": "Informatics in Medicine Unlocked",
        "keywords": "Wound measurement, Assessment, Tissue classification, Artificial intelligence, Computer vision",
        "pages": "101185",
        "title": "Clinical validation of computer vision and artificial intelligence algorithms for wound measurement and tissue classification in wound care",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S2352914823000278",
        "volume": "37",
        "year": "2023"
    },
    "ROY2023101919": {
        "abstract": "Objective. With climatic instability, various ecological disturbances, and human actions threaten the existence of various endangered wildlife species. Therefore, an up-to-date accurate and detailed detection process plays an important role in protecting biodiversity losses, conservation, and ecosystem management. Current state-of-the-art wildlife detection models, however, often lack superior feature extraction capability in complex environments, limiting the development of accurate and reliable detection models. Method. To this end, we present WilDect-YOLO, a deep learning (DL)-based automated high-performance detection model for real-time endangered wildlife detection. In the model, we introduce a residual block in the CSPDarknet53 backbone for strong and discriminating deep spatial features extraction and integrate DenseNet blocks to improve in preserving critical feature information. To enhance receptive field representation, preserve fine-grain localized information, and improve feature fusion, a Spatial Pyramid Pooling (SPP) and modified Path Aggregation Network (PANet) have been implemented that results in superior detection under various challenging environments. Results. Evaluating the model performance in a custom endangered wildlife dataset considering high variability and complex backgrounds, WilDect-YOLO obtains a mean average precision (mAP) value of 96.89%, F1-score of 97.87%, and precision value of 97.18% at a detection rate of 59.20 FPS outperforming current state-of-the-art models. Significance. The present research provides an effective and efficient detection framework addressing the shortcoming of existing DL-based wildlife detection models by providing highly accurate species-level localized bounding box prediction. Current work constitutes a step toward a non-invasive, fully automated animal observation system in real-time in-field applications.",
        "author": "Arunabha M. Roy and Jayabrata Bhaduri and Teerath Kumar and Kislay Raj",
        "doi": "https://doi.org/10.1016/j.ecoinf.2022.101919",
        "issn": "1574-9541",
        "journal": "Ecological Informatics",
        "keywords": "Endangered wildlife detection, You only look once (YOLOv4) algorithm, Object detection (OD), Computer vision, Deep learning (DL), Wildlife preservation",
        "pages": "101919",
        "title": "WilDect-YOLO: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S1574954122003697",
        "volume": "75",
        "year": "2023"
    },
    "TIEN2022100038": {
        "abstract": "In favorable climates and building types, employing natural ventilation can lead to significant energy savings and health benefits. However, in cold climates or conditions, the use of natural ventilation could result in significant heat loss and, consequently, excessive heating bills. This is further exacerbated when windows are left unintentionally open by occupants during the heating season, causing unnecessary energy consumption and wastage, which compromises the heating, ventilation and air-conditioning (HVAC) efficiency. Occupant behavior influences and shapes the building's energy use and indoor environment quality. In particular, the occupant's interaction with the building and its elements, such as window openings, has a considerable effect on the air change rate and the thermal load for ventilation. Studies have shown that real-time occupancy information can improve the operation of HVAC, lighting and utilization of building zones or spaces by coupling it with demand-driven control and occupant-centric strategies. The present study introduces a computer vision and deep learning-based detection approach for the real-time monitoring and recognition of the opening and closing of windows. The study aims to use the detection approach to reduce the energy demand by correctly controlling the HVAC or alerting the building users/operators during periods when windows are left open, minimizing the unwanted air change rates and heating or cooling loads. The study will take an in-depth look into the performance of the detection model, in particular, the influence of data curation, labelling and training employed. Four types of window detectors were configured and evaluated based on the detection of a set of windows within a case study building, which will help seek the most accurate detection and recognition of window opening status. The impact of the detection method on building energy demand was investigated through a series of building energy simulation (BES) scenarios. Simulations were conducted using predefined fixed profiles, along with the window detection and \u2018actual\u2019 profiles. The study has shown that the detection and recognition ability of the models ultimately influenced the prediction of the ventilation heat loss and heating energy demand.",
        "author": "Paige Wenbin Tien and Shuangyu Wei and John Kaiser Calautit and Jo Darkwa and Christopher Wood",
        "doi": "https://doi.org/10.1016/j.cles.2022.100038",
        "issn": "2772-7831",
        "journal": "Cleaner Energy Systems",
        "keywords": "Computer vision, Deep learning, Detection, Ventilation heat loss, Window opening",
        "pages": "100038",
        "title": "Enhancing the detection performance of a vision-based window opening detector",
        "type": "article",
        "url": "https://www.sciencedirect.com/science/article/pii/S277278312200036X",
        "volume": "3",
        "year": "2022"
    },
    "s23020798": {
        "abstract": "There is great interest in automatically detecting road weather and understanding its impacts on the overall safety of the transport network. This can, for example, support road condition-based maintenance or even serve as detection systems that assist safe driving during adverse climate conditions. In computer vision, previous work has demonstrated the effectiveness of deep learning in predicting weather conditions from outdoor images. However, training deep learning models to accurately predict weather conditions using real-world road-facing images is difficult due to: (1) the simultaneous occurrence of multiple weather conditions; (2) imbalanced occurrence of weather conditions throughout the year; and (3) road idiosyncrasies, such as road layouts, illumination, and road objects, etc. In this paper, we explore the use of a focal loss function to force the learning process to focus on weather instances that are hard to learn with the objective of helping address data imbalances. In addition, we explore the attention mechanism for pixel-based dynamic weight adjustment to handle road idiosyncrasies using state-of-the-art vision transformer models. Experiments with a novel multi-label road weather dataset show that focal loss significantly increases the accuracy of computer vision approaches for imbalanced weather conditions. Furthermore, vision transformers outperform current state-of-the-art convolutional neural networks in predicting weather conditions with a validation accuracy of 92% and an F1-score of 81.22%, which is impressive considering the imbalanced nature of the dataset.",
        "article-number": "798",
        "author": "Samo, Madiha and Mafeni Mase, Jimiama Mosima and Figueredo, Grazziela",
        "doi": "10.3390/s23020798",
        "issn": "1424-8220",
        "journal": "Sensors",
        "number": "2",
        "pubmedid": "36679596",
        "title": "Deep Learning with Attention Mechanisms for Road Weather Detection",
        "type": "Article",
        "url": "https://www.mdpi.com/1424-8220/23/2/798",
        "volume": "23",
        "year": "2023"
    }
}});